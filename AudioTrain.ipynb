{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing.dummy as mp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down', 'no', 'up', 'yes']\n"
     ]
    }
   ],
   "source": [
    "main_dir = 'ML_TACTIGON/customTSkin/data/audiodati'\n",
    "folders = [folder for folder in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, folder))]\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 16000\n",
    "\n",
    "def load_audio(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=sampling_rate)\n",
    "    label = os.path.basename(os.path.dirname(file_path))  \n",
    "    filename = os.path.basename(file_path)                \n",
    "    return filename, label, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loading data from only the first 7 folders\n",
    "file_paths = []\n",
    "for folder in folders[:4]:\n",
    "    folder_files = [os.path.join(main_dir, folder, file) for file in os.listdir(os.path.join(main_dir, folder)) if file.endswith('.wav')]\n",
    "    file_paths.extend(folder_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(os.cpu_count()) as pool:  \n",
    "    audio_data = pool.map(load_audio, file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizing results into a DataFrame with filename, label, and audio data\n",
    "audio_df = pd.DataFrame(audio_data, columns=['filename', 'label', 'audio_data'])\n",
    "audio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting dataframe shape\n",
    "audio_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function for extracting MFCCs for a single audio sample\n",
    "def extract_mfcc_parallel(audio_data, sr, n_mfcc=13):\n",
    "    return librosa.feature.mfcc(y=audio_data, sr=sampling_rate, n_mfcc=n_mfcc).T\n",
    "\n",
    "# preparing data for parallel processing\n",
    "audio_data_list = audio_df['audio_data'].tolist()\n",
    "\n",
    "# using thread-based multiprocessing to extract MFCCs in parallel\n",
    "with mp.Pool(os.cpu_count()) as pool:\n",
    "    mfcc_features = pool.starmap(extract_mfcc_parallel, [(audio, sampling_rate) for audio in audio_data_list])\n",
    "\n",
    "# adding the extracted MFCC features to the DataFrame\n",
    "audio_df['mfcc'] = mfcc_features\n",
    "audio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of arrays (MFCCs) into a 2D array for standardization\n",
    "mfcc_flattened = np.concatenate(audio_df['mfcc'].values, axis=0)\n",
    "\n",
    "# initializing the scaler and fit only on the flattened MFCCs (training data only)\n",
    "scaler = StandardScaler().fit(mfcc_flattened)\n",
    "\n",
    "# function to apply scaler to each sample\n",
    "def scale_features(features, scaler):\n",
    "    scaled_features = []\n",
    "    for feature in features:\n",
    "        scaled = scaler.transform(feature)  # standardize each sample\n",
    "        scaled_features.append(scaled)\n",
    "    return np.array(scaled_features, dtype=object)\n",
    "\n",
    "# applying scaling to MFCC features\n",
    "audio_df['scaled_mfcc'] = scale_features(audio_df['mfcc'].values, scaler)\n",
    "audio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting labels\n",
    "audio_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting an audio sample\n",
    "audio_sample = audio_df[audio_df['label'] == 'backward']['audio_data'].iloc[0]\n",
    "\n",
    "# creating a figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(13, 7))\n",
    "\n",
    "# 1. Waveform\n",
    "librosa.display.waveshow(audio_sample, sr=sampling_rate, ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Waveform\")\n",
    "axes[0, 0].set_xlabel(\"Time (s)\")\n",
    "axes[0, 0].set_ylabel(\"Amplitude\")\n",
    "\n",
    "# 2. Spectrogram\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_sample)), ref=np.max)\n",
    "librosa.display.specshow(D, sr=sampling_rate, x_axis='time', y_axis='log', ax=axes[0, 1], cmap='viridis')\n",
    "axes[0, 1].set_title(\"Spectrogram\")\n",
    "axes[0, 1].set_xlabel(\"Time (s)\")\n",
    "axes[0, 1].set_ylabel(\"Frequency (Hz)\")\n",
    "\n",
    "# 3. MFCC\n",
    "mfccs = librosa.feature.mfcc(y=audio_sample, sr=sampling_rate, n_mfcc=13)\n",
    "librosa.display.specshow(mfccs, x_axis='time', sr=sampling_rate, ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"MFCC\")\n",
    "axes[1, 0].set_xlabel(\"Time (s)\")\n",
    "axes[1, 0].set_ylabel(\"MFCC Coefficients\")\n",
    "fig.colorbar(librosa.display.specshow(mfccs, x_axis='time', sr=sampling_rate, ax=axes[1, 0]), ax=axes[1, 0])\n",
    "\n",
    "# 4. Chromagram\n",
    "chroma = librosa.feature.chroma_stft(y=audio_sample, sr=sampling_rate)\n",
    "librosa.display.specshow(chroma, y_axis='chroma', x_axis='time', sr=sampling_rate, ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"Chromagram\")\n",
    "axes[1, 1].set_xlabel(\"Time (s)\")\n",
    "axes[1, 1].set_ylabel(\"Pitch Class\")\n",
    "fig.colorbar(librosa.display.specshow(chroma, y_axis='chroma', x_axis='time', sr=sampling_rate, ax=axes[1, 1]), ax=axes[1, 1])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(audio_df['scaled_mfcc'], audio_df['label'], test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = pad_sequences(X_train, padding='post')\n",
    "X_test_padded = pad_sequences(X_test, padding='post')\n",
    "X_val_padded = pad_sequences(X_val, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "y_val = label_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_padded.shape[1], X_train_padded.shape[2])),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(audio_df['label'].unique()), activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# compiling the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# define the ModelCheckpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    'model_A.keras',           # filepath to save the model\n",
    "    monitor='val_accuracy',       # metric to monitor\n",
    "    save_best_only=True,          # save only the best model\n",
    "    mode='max',                   # mode to determine if the monitored quantity is improving\n",
    "    verbose=1                     # verbosity mode\n",
    ")\n",
    "\n",
    "# Train the model with the callback\n",
    "history = model.fit(\n",
    "    X_train_padded, \n",
    "    y_train, \n",
    "    epochs=50, \n",
    "    validation_data=(X_val_padded, y_val),\n",
    "    callbacks=[checkpoint_callback]  # include the callback here\n",
    ")\n",
    "\n",
    "# loading the best model after training\n",
    "best_model = tf.keras.models.load_model('model_A.keras')\n",
    "\n",
    "# evaluating the best model on training and validation data\n",
    "train_loss, train_accuracy = best_model.evaluate(X_train_padded)\n",
    "val_loss, val_accuracy = best_model.evaluate(y_val)\n",
    "\n",
    "# print the model evaluation results\n",
    "print('------------------------------')\n",
    "print('------Best Model Summary------')\n",
    "print(f\"Training Loss: {train_loss:.4f}, \\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}, \\nValidation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the improved model\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_padded.shape[1], X_train_padded.shape[2])),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(128, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(256, kernel_size=3, activation='relu'),  # Increased complexity\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),  # Increased neurons\n",
    "    Dropout(0.5),\n",
    "    Dense(len(audio_df['label'].unique()), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiling the model with Adam optimizer\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define the ModelCheckpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    'model_B.keras',           # filepath to save the model\n",
    "    monitor='val_accuracy',       # metric to monitor\n",
    "    save_best_only=True,         # save only the best model\n",
    "    mode='max',                   # mode to determine if the monitored quantity is improving\n",
    "    verbose=1                     # verbosity mode\n",
    ")\n",
    "\n",
    "# Add ReduceLROnPlateau callback to adjust learning rate\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',          # metric to monitor\n",
    "    factor=0.5,                  # factor by which the learning rate will be reduced\n",
    "    patience=5,                  # number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-6,                 # lower bound on the learning rate\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model with the callbacks\n",
    "history = model.fit(\n",
    "    X_train_padded, \n",
    "    y_train, \n",
    "    epochs=50, \n",
    "    validation_data=(X_val_padded, y_val),\n",
    "    callbacks=[checkpoint_callback, reduce_lr_callback]  # include both callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "# Assuming X_test_padded is your padded test data and y_test are the true labels\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)  # Convert one-hot encoded predictions to label indices\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "# Get the label names from the label encoder\n",
    "label_names = label_encoder.classes_\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AUDIOTRAIN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
