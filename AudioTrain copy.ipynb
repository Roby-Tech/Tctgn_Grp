{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = \"ML_TACTIGON/customTSkin/data/audiodati\"\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 1.0\n",
    "NUM_CLASSES = 4\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down: 3917 file .wav\n",
      "no: 3941 file .wav\n",
      "up: 3723 file .wav\n",
      "yes: 4043 file .wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_wav_files(directory):\n",
    "    counts = {}\n",
    "\n",
    "    # Itera attraverso le sottocartelle nella directory principale\n",
    "    for subfolder in os.listdir(directory):\n",
    "        subfolder_path = os.path.join(directory, subfolder)\n",
    "\n",
    "        # Verifica che sia una directory\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            # Conta i file con estensione .wav nella sottocartella\n",
    "            wav_files = [file for file in os.listdir(subfolder_path) if file.endswith('.wav')]\n",
    "            counts[subfolder] = len(wav_files)\n",
    "\n",
    "    return counts\n",
    "\n",
    "# Percorso della directory principale\n",
    "directory = \"ML_TACTIGON/customTSkin/data/audiodati\"\n",
    "counts = count_wav_files(directory)\n",
    "\n",
    "# Stampa il numero di file .wav per ogni sottocartella\n",
    "for subfolder, count in counts.items():\n",
    "    print(f\"{subfolder}: {count} file .wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, sample_rate, duration):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.num_samples = int(sample_rate * duration)\n",
    "        self.classes = sorted(os.listdir(data_dir))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            class_dir = os.path.join(data_dir, cls)\n",
    "            for file in os.listdir(class_dir):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    self.data.append(os.path.join(class_dir, file))\n",
    "                    self.labels.append(self.class_to_idx[cls])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)(waveform)\n",
    "\n",
    "        if waveform.shape[1] < self.num_samples:\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, self.num_samples - waveform.shape[1]))\n",
    "        else:\n",
    "            waveform = waveform[:, :self.num_samples]\n",
    "\n",
    "        return waveform, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(16)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(32)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * (SAMPLE_RATE // 8), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.batch_norm2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.batch_norm3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs, patience=3):\n",
    "    best_val_loss = float('inf')  # Migliore validation loss osservata\n",
    "    counter = 0  # Contatore per la pazienza\n",
    "    best_model_state = None  # Per salvare il miglior modello\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Fase di training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Fase di validazione\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {running_loss / len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2%}\")\n",
    "\n",
    "        # Early stopping: verifica se la validation loss migliora\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            best_model_state = model.state_dict()  # Salva lo stato del miglior modello\n",
    "        else:\n",
    "            counter += 1\n",
    "            print(f\"Early stopping counter: {counter}/{patience}\")\n",
    "\n",
    "        # Interrompi il training se il contatore supera la pazienza\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    # Ripristina il miglior modello trovato\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creazione dataset completo\n",
    "dataset = AudioDataset(DATA_DIR, SAMPLE_RATE, DURATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Divisione del dataset in train e test set (80% train, 20% test)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione dei DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione modello\n",
    "model = AudioClassifier(NUM_CLASSES).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.9783, Val Loss: 0.6799, Val Acc: 70.58%\n",
      "Epoch 2/10, Train Loss: 0.6978, Val Loss: 0.6083, Val Acc: 72.76%\n",
      "Epoch 3/10, Train Loss: 0.5957, Val Loss: 0.5293, Val Acc: 78.87%\n",
      "Epoch 4/10, Train Loss: 0.5227, Val Loss: 0.5264, Val Acc: 80.31%\n",
      "Epoch 5/10, Train Loss: 0.4539, Val Loss: 0.5102, Val Acc: 80.70%\n",
      "Epoch 6/10, Train Loss: 0.3980, Val Loss: 0.5067, Val Acc: 81.18%\n",
      "Epoch 7/10, Train Loss: 0.3643, Val Loss: 0.5345, Val Acc: 79.99%\n",
      "Early stopping counter: 1/3\n",
      "Epoch 8/10, Train Loss: 0.3189, Val Loss: 0.5771, Val Acc: 79.99%\n",
      "Early stopping counter: 2/3\n",
      "Epoch 9/10, Train Loss: 0.2917, Val Loss: 0.5371, Val Acc: 81.34%\n",
      "Early stopping counter: 3/3\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AudioClassifier(\n",
       "  (conv1): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (batch_norm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=128000, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, DEVICE, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AUDIODETECT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
